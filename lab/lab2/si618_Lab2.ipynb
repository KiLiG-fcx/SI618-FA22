{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_ict6b-qxLD"
   },
   "source": [
    "Let's learn more about pandas! Let's load the appropriate library\n",
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MHMdV0wyFX6X"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gU76-U0ftZWw"
   },
   "source": [
    "<font color='red'>To-do:</font> Load nfl_football_profiles.json to pandas df named nfl and display the first 10 rows(5 points)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ItOuid97rc8"
   },
   "source": [
    "Lets clean up the data:\n",
    "Let's remove all the commas in the current_salary column in a new column called salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nCkaEzn_7qnm"
   },
   "outputs": [],
   "source": [
    "nfl['salary'] = nfl['current_salary'].str.replace(',', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8KOiOlqO8iLQ"
   },
   "source": [
    "Let's clean this up further by converting the data type to a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nwE5RCSZ8RJq"
   },
   "outputs": [],
   "source": [
    "nfl['salary'] = nfl['salary'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXgeFIlK8q4F"
   },
   "outputs": [],
   "source": [
    "nfl['salary'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5PUTFtJ87aX"
   },
   "source": [
    "<font color='red'>To-do:</font> Remove all NaN values in salary from the nfl df into a new df called nfl_salary_cleaned (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2qNZJaFe-y7N"
   },
   "source": [
    "# Dummy variables and discretizing salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9C1CVM9J_Adr"
   },
   "source": [
    "<font color='red'>To-do:</font> Using the same NFL dataset, lets discretize the salaries. Let's create bins by splitting salaries into 3 equally sized groups (hint: use pd.Series.quantiles and np.linspace) (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q_6JTAk6AHiX"
   },
   "source": [
    "Using these bins that we set, we can set labels to them and have them categorized by that bin by using pd.cut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1599755754780,
     "user": {
      "displayName": "Kevin Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-Bzlj5PkYle1VXcuH3tD0oB_eYMp8TFinoIcH=s64",
      "userId": "17607607416881687332"
     },
     "user_tz": 240
    },
    "id": "4zW1cVoG_WB6",
    "outputId": "74ddc7e9-0c21-4491-aaec-bb17ae2d02b5"
   },
   "outputs": [],
   "source": [
    "pd.cut(nfl_salary_cleaned['salary'],bins,labels=['small', 'medium', 'large'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mr5XCFWnANYN"
   },
   "source": [
    "Similarly, we can create dummy variables to categorize our labels and bins into an organized dataframe by using pd.get_dummies(). It will display whether a player fits into our labels based on their salary bin range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mtcXsec9_mlA"
   },
   "outputs": [],
   "source": [
    "dummy_variable = pd.get_dummies(pd.cut(nfl_salary_cleaned['salary'],bins,labels=['small', 'medium', 'large']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1599755759592,
     "user": {
      "displayName": "Kevin Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-Bzlj5PkYle1VXcuH3tD0oB_eYMp8TFinoIcH=s64",
      "userId": "17607607416881687332"
     },
     "user_tz": 240
    },
    "id": "UKxoBGbu_res",
    "outputId": "7941c3c2-82f6-4cef-e43d-656191a8ba44"
   },
   "outputs": [],
   "source": [
    "dummy_variable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping and Concatenating Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>To-do:</font> Let's practice scraping html tables. Can you scrape the HTML tables of James Bond movies made by Eon Productions and the James Bond movies not made by Eon Productions from wikipedia (link: https://en.wikipedia.org/wiki/List_of_James_Bond_films)? Store the 2 scraped dataframes into 'eon' and 'non_eon'. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up the data: let's drop some rows and columns of the datasets and reduce the levels of column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eon = eon.iloc[:-1,:-6]\n",
    "eon.columns = eon.columns.droplevel(-1)\n",
    "eon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_eon = non_eon.iloc[:-1,:-6]\n",
    "non_eon.columns = non_eon.columns.droplevel(-1)\n",
    "non_eon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>To-do:</font> Concatenate Eon films and Non-Eon films on rows, but ignore indexes. Show all rows. Store the new dataframe to \"james_bond\" (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>To-do:</font> Count the number of times each actor played James Bond. How many actors in total has played James Bond? Which actor(s) has played James Bond in the most films? Please **do not** use groupby to solve this question. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xd9q8cM3FG_H"
   },
   "source": [
    "# Joining and Merging Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h6RG15tRGltz"
   },
   "source": [
    "Now let's practice joining tables. Load in title.ratings.tsv and title.basics.tsv. <font color='red'>These files are included in the zipped file you downloaded with your lab instructions. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vllwqTvcGwkN"
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('title.ratings.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1600097831675,
     "user": {
      "displayName": "Kevin Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-Bzlj5PkYle1VXcuH3tD0oB_eYMp8TFinoIcH=s64",
      "userId": "17607607416881687332"
     },
     "user_tz": 240
    },
    "id": "kIjePgntHLuI",
    "outputId": "82b665fe-e3ae-4a3c-b862-4a0febf6d0f9"
   },
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26728,
     "status": "ok",
     "timestamp": 1600097860039,
     "user": {
      "displayName": "Kevin Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-Bzlj5PkYle1VXcuH3tD0oB_eYMp8TFinoIcH=s64",
      "userId": "17607607416881687332"
     },
     "user_tz": 240
    },
    "id": "ebJKDdrmLrEV",
    "outputId": "65603d38-29b2-4890-a1ba-dca1d55f5124"
   },
   "outputs": [],
   "source": [
    "titles = pd.read_csv('title.basics.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24873,
     "status": "ok",
     "timestamp": 1600097860041,
     "user": {
      "displayName": "Kevin Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-Bzlj5PkYle1VXcuH3tD0oB_eYMp8TFinoIcH=s64",
      "userId": "17607607416881687332"
     },
     "user_tz": 240
    },
    "id": "Ds-m6jZDLymQ",
    "outputId": "51f02e43-26b9-47a5-8e8a-ffc397acd492"
   },
   "outputs": [],
   "source": [
    "titles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Guk9OUcMMGpu"
   },
   "source": [
    "Notice that on titles and ratings, tconst is the same. We can therefore join these two dataframe using that column as index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vrLe3kJAMQpn"
   },
   "source": [
    "<font color='red'>To-do:</font> Merge ratings and titles in a dataframe called imdb_info on tconst. In a markdown cell, explain what kind of join this is and why you chose to use it. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_9mVmVanTFsi"
   },
   "source": [
    "<font color='red'>To-do:</font> In a markdown cell, explain the difference between an inner join, outer join, right join, and left join. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iX4_9PsjR6xq"
   },
   "source": [
    "<font color='red'>To-do:</font> What is the difference between pd.concat and pd.merge? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lsm9e-H2nWyh"
   },
   "source": [
    "# Exploding, manipulation and grouping practice, YAY!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SaWUa4fSneeu"
   },
   "source": [
    "Let's look at the genres in our imdb_info df. Use explode function to create a row per genre, title pair. (Similar to Q14 in HW1). There also appears to be '\\N' tacked onto each of the cells. Let's also get rid of that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3188,
     "status": "error",
     "timestamp": 1599761694272,
     "user": {
      "displayName": "Kevin Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-Bzlj5PkYle1VXcuH3tD0oB_eYMp8TFinoIcH=s64",
      "userId": "17607607416881687332"
     },
     "user_tz": 240
    },
    "id": "xHh9YdlBnd-4",
    "outputId": "2ea860a6-cbfa-4b20-e266-9ea2e759efe6"
   },
   "outputs": [],
   "source": [
    "imdb_info.genres = imdb_info.genres.str.extract(r'([a-zA-Z,]*)')\n",
    "imdb_info.loc[:,\"genre_list\"] = imdb_info.apply(lambda x: str(x[\"genres\"]).split(\",\"),axis=1)\n",
    "df_genre = imdb_info.explode(\"genre_list\")\n",
    "df_genre.rename(columns={'genre_list':'genre'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tPQyN-vW8F9G"
   },
   "source": [
    "Looking at runtimeMinutes, we see that its type is object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1599761696758,
     "user": {
      "displayName": "Kevin Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-Bzlj5PkYle1VXcuH3tD0oB_eYMp8TFinoIcH=s64",
      "userId": "17607607416881687332"
     },
     "user_tz": 240
    },
    "id": "cpsIQYDx8B2s",
    "outputId": "4116e414-b911-462e-a994-67b332e3882f"
   },
   "outputs": [],
   "source": [
    "df_genre['runtimeMinutes'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RPpK42-F9_mX"
   },
   "source": [
    "We want to fix the data up a bit more. There appears to be '\\\\N' tacked onto the end of runtimeMinutes. Let's just grab the number in each of the cells by using some regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uS86YNg6GRtQ"
   },
   "source": [
    "<font color='red'>To-do:</font> Extract the numeric values from the runtimeMinutes column. (HINT: use .str.extract()) (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hhmdqtW28480"
   },
   "source": [
    "<font color='red'>To-do:</font> Change the object dtype of runtimeMinutes to float. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QKIUGva8EuCe"
   },
   "source": [
    "Let's practice using some groupby on the dataset. Let's look at the df by grouping by the average rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 666,
     "status": "ok",
     "timestamp": 1599761710694,
     "user": {
      "displayName": "Kevin Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-Bzlj5PkYle1VXcuH3tD0oB_eYMp8TFinoIcH=s64",
      "userId": "17607607416881687332"
     },
     "user_tz": 240
    },
    "id": "GiO2RRyLEzYo",
    "outputId": "0ad9a335-5d8d-427b-ec66-66e22cf183d7"
   },
   "outputs": [],
   "source": [
    "df_genre.groupby('averageRating').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UttGyKpX7VIe"
   },
   "source": [
    "<font color='red'>To-do:</font> Find the average runtime for averageRating for each genre. Find the first quartile. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T8ZHXqh978Sx"
   },
   "source": [
    "<font color='red'>To-do:</font> What is the total number of votes for each genre, for each start year? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on Child Opportunity Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's analyse the child opportunity dataset from the lecture with the tools we've learnt upto now. But, first let's load the Child Opportunity Index json file from the lecture. Column descriptions can be found at this [link](https://data.diversitydatakids.org/dataset/coi20-child-opportunity-index-2-0-database/resource/f16fff12-b1e5-4f60-85d3-3a0ededa30a0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opportunity = pd.read_json('raw.json')\n",
    "opportunity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>To-do:</font>  Create a dataframe that shows the min, average and maximum median household income in census tracts that belong to a top 100 metropolitan area and those that don't (hint: use groupby and apply to do this) (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>To-do:</font> Create a new column in the dataframe by bucketing the median household income into 3 equal sized in terms of number of rows groups (hint use pd.Series.quantile function) \"low_mhe\", \"medium_mhe\", and \"high_mhe\".  Create another column by bucketing the employment rate into 3 equal sized groups \"low_emp\" \"medium_emp\", and \"high_emp\" (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>To-do:</font> Now create a table such that the index corresponds to whether a census tract belongs to top 100 metropolitan area, the columns correspond to the median household income bucket and the values correspond to average third-grade reading proficiency. As a challenge, consider what you may be able say about (i) the relationship between household income and reading proficiency of children as a whole and (ii) how this relationship differs between big cities and elsewehere? (iii) how about the relationship between the employment rate and the average early childhood education enrollment? How this relationship differs between big cities and elsewehere? You may create another pivot table for this question.(15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "h_ict6b-qxLD",
    "2qNZJaFe-y7N",
    "xd9q8cM3FG_H",
    "Lsm9e-H2nWyh"
   ],
   "name": "si618_Lab2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "96890b704896806e1b70ec3468f2aa0caaad9a2f3cc76dad48e7dacf13d55fab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
